{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educated-response",
   "metadata": {},
   "source": [
    "# Feature importance analysis\n",
    "for Tree Mortality Prediction based on Growth Patterns \n",
    "using Machine Learning Models  \n",
    "  \n",
    "__Iulia Iordanescu__, Acton Boxborough Regional HS, 24iordanescui@abschools.org  \n",
    "__Michael Dietze__, Department of Earth & Environment, Boston University, dietze@bu.edu  \n",
    "\n",
    "### Harvard Forest Ecology Symposium\n",
    "#### March 16-17, 2021  \n",
    "   \n",
    "   \n",
    "data: [HF213](https://harvardforest1.fas.harvard.edu/exist/apps/datasets/showData.html?id=HF213)\n",
    "\n",
    "Use classification algorithms to predict A(live) or D(ead) labels in __mortality13__ and __mortality14__ columns using these features:  \n",
    " - spp: USDA Plants database species code  \n",
    " - dbh09: diameter at Breast Height (1.4m) in year 2009 (unit: centimeter / missing value: NA)  \n",
    " - dbh11: diameter at Breast Height (1.4m) in year 2011 (unit: centimeter / missing value: NA)  \n",
    " - dbh12: diameter at Breast Height (1.4m) in year 2012 (unit: centimeter / missing value: NA)  \n",
    " - dbh13: diameter at Breast Height (1.4m) in year 2013 (unit: centimeter / missing value: NA)  \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "##### Tree mortality prediction based on growth patterns using Machine Learning\n",
    "Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pathlib, shutil, platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    StratifiedShuffleSplit,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_COUNT = 10\n",
    "TRAIN_DATA = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/conda/bin/conda install -c anaconda seaborn pandas scikit-learn -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "# !ls -la ./../data/hrvardf/HF213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFileName='hf213-01-hf-inventory.csv'\n",
    "dataPathFull= pathlib.Path('./../data/harvardf/HF213') / dataFileName\n",
    "myData = pd.read_csv(str(dataPathFull)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData.shape\n",
    "myData.head(2)\n",
    "myData.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData.info()\n",
    "myData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myData.dropna()\n",
    "# myData.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic descriptive statistics for numeric columns:\n",
    "myData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=myData.corr()\n",
    "plt.figure(figsize = (9, 7))\n",
    "sns.heatmap(corr, cmap=\"RdBu\",\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myData.groupby('spp').size()\n",
    "myCols = ['spp', 'mortality14', 'dmg14']\n",
    "myData[myCols[0]].value_counts(dropna=False) \n",
    "myData[myCols[1]].value_counts(dropna=False)\n",
    "myData[myCols[2]].value_counts(dropna=False)\n",
    "# myData.pivot_table(index = [myCols[0]]\n",
    "#                    , columns = myCols[1]\n",
    "#                    , values =  myCols[2]\n",
    "#                    , aggfunc=np.sum, fill_value=0)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x= myData['spp'],label=\"spp Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData['spp'].value_counts(dropna=False) \n",
    "removeSPP = myData['spp'].value_counts(dropna=False).loc[lambda x : x<MINIMUM_COUNT].index.tolist()\n",
    "removeSPP\n",
    "\n",
    "# filteredData = myData.replace(dict.fromkeys(removeSPP, 'TooFew'))\n",
    "# filteredData['spp'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumn_01=['spp', 'dbh09', 'dbh11', 'dbh12']\n",
    "# featureColumn_01=[ 'dbh09', 'dbh11', 'dbh12']\n",
    "labelColumn_01 = 'mortality13'\n",
    "featureColumn_02=['spp', 'dbh09', 'dbh11', 'dbh12', 'dbh13']\n",
    "# featureColumn_02=['dbh09', 'dbh11', 'dbh12', 'dbh13']\n",
    "labelColumn_02 = 'mortality14' \n",
    "\n",
    "labelColumn = labelColumn_02\n",
    "featureColumn = featureColumn_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(featureColumn+[labelColumn]))\n",
    "filteredDataML = myData[sorted(set(featureColumn+[labelColumn]))]\n",
    "\n",
    "# filteredDataML.shape\n",
    "# filteredDataML.head()\n",
    "# filteredDataML[labelColumn].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(filteredDataML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "features_cols = [x for x in featureColumn if x != 'spp']\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer.fit(filteredDataML[features_cols])\n",
    "imputed = imputer.transform(filteredDataML[features_cols])\n",
    "filteredDataML[features_cols]=imputer.transform(filteredDataML[features_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDataML.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  itertools import combinations\n",
    "list(combinations(sorted(features_cols), 2))\n",
    "[print(col1, col2) for col1, col2 in combinations(sorted(features_cols), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDataML.columns\n",
    "# col1= trainData.columns[0]\n",
    "# col2= trainData.columns[1]\n",
    "\n",
    "# trainData1 = trainData.join(pd.DataFrame(((trainData[col2]/(trainData[col1]) -1)*100).rename(f'{col1}to{col2}')))\n",
    "\n",
    "filteredDataML = filteredDataML.join(\n",
    "    pd.concat([(((filteredDataML[col2]/(filteredDataML[col1]) -1)*100)\n",
    "                .rename(f'{col2}to{col1}'))\n",
    "               for col1, col2 in combinations(sorted(features_cols), 2)], 1))\n",
    "# testData = testData.join(\n",
    "#     pd.concat([(((testData[col2]/(testData[col1]) -1)*100)\n",
    "#                 .rename(f'{col2}to{col1}'))\n",
    "#                for col1, col2 in combinations(sorted(numCols), 2)], 1))\n",
    "# validationData = validationData.join(\n",
    "#     pd.concat([(((validationData[col2]/(validationData[col1]) -1)*100)\n",
    "#                 .rename(f'{col2}to{col1}'))\n",
    "#                for col1, col2 in combinations(sorted(numCols), 2)], 1))\n",
    "\n",
    "# trainData.head(2)\n",
    "# testData.head(2)\n",
    "# validationData.head(2)\n",
    "filteredDataML.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_13_14 = pd.crosstab(index=myData['mortality14']\n",
    "                            ,columns=myData['mortality13']\n",
    "                            , margins=True)\n",
    "mortality_13_14\n",
    "\n",
    "mortality_spp = pd.crosstab(index=myData['spp']\n",
    "                            ,columns=myData['mortality14']\n",
    "                            , margins=True)\n",
    "mortality_spp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=filteredDataML.corr()\n",
    "corr\n",
    "plt.figure(figsize = (9, 7))\n",
    "sns.heatmap(corr, cmap=\"RdBu\",\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDataML[labelColumn].value_counts(dropna=False)\n",
    "\n",
    "filteredDataML = filteredDataML[filteredDataML[labelColumn].isin(['D', 'A'])]\n",
    "filteredDataML.shape\n",
    "filteredDataML.head()\n",
    "\n",
    "\n",
    "filteredDataML[labelColumn].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc_lbl_df = pd.DataFrame((oh_encoder.fit_transform(filteredDataML[['spp']])).toarray())\n",
    "# enc_lbl_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py\n",
    "\n",
    "catCols = filteredDataML.columns[filteredDataML.dtypes == 'O']\n",
    "numCols = filteredDataML.columns[filteredDataML.dtypes == 'float64']\n",
    "catCols\n",
    "numCols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filteredDataML.loc[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratifySplit = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_DATA, random_state=1)\n",
    "\n",
    "trainIdx, tstIdx = next(stratifySplit.split(filteredDataML, filteredDataML[labelColumn]))\n",
    "# print(\"\\n Train:\", sorted(trainIdx))\n",
    "len(trainIdx)\n",
    "len(tstIdx)\n",
    "\n",
    "filteredDataML.loc[filteredDataML.index.intersection(filteredDataML.index[trainIdx])].shape\n",
    "filteredDataML[filteredDataML.index.isin(filteredDataML.index[trainIdx])].shape\n",
    "aa=filteredDataML.loc[filteredDataML.index.intersection(filteredDataML.index[tstIdx])]\n",
    "aa.shape\n",
    "stratifySplit = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_DATA, test_size=1-TRAIN_DATA, random_state=1)\n",
    "testIdx, validationIdx = next(stratifySplit.split(aa,  aa[labelColumn]))\n",
    "\n",
    "len(testIdx)\n",
    "len(validationIdx)\n",
    "filteredDataML.shape\n",
    "\n",
    "# testIdx=tstIdx[testIdx]\n",
    "# validationIdx=tstIdx[validationIdx]\n",
    "\n",
    "# print(\"\\n Test:\", sorted(testIdx))\n",
    "# print(\"\\nValidation:\", sorted(validationIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData=filteredDataML.loc[filteredDataML.index.intersection(filteredDataML.index[trainIdx]),:]\n",
    "testData=aa.loc[aa.index.intersection(aa.index[testIdx]),:]\n",
    "validationData = aa.loc[aa.index.intersection(aa.index[validationIdx]),:]\n",
    "\n",
    "filteredDataML[labelColumn].value_counts(dropna=False)\n",
    "trainData[labelColumn].value_counts(dropna=False) \n",
    "testData[labelColumn].value_counts(dropna=False) \n",
    "validationData[labelColumn].value_counts(dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinalEncoder = OrdinalEncoder()\n",
    "ordinalEncoder.fit(filteredDataML[catCols])\n",
    "ordinalEncoder.categories_\n",
    "\n",
    "trainData[catCols] = ordinalEncoder.transform(trainData[catCols])\n",
    "testData[catCols] = ordinalEncoder.transform(testData[catCols])\n",
    "validationData[catCols] = ordinalEncoder.transform(validationData[catCols])\n",
    "\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "# imputer.fit(trainData[featureColumn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData[featureColumn] = imputer.transform(trainData[featureColumn])\n",
    "# testData[featureColumn] = imputer.transform(testData[featureColumn])\n",
    "# validationData[featureColumn] = imputer.transform(validationData[featureColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from  itertools import combinations\n",
    "# trainData.columns\n",
    "# # col1= trainData.columns[0]\n",
    "# # col2= trainData.columns[1]\n",
    "\n",
    "# # trainData1 = trainData.join(pd.DataFrame(((trainData[col2]/(trainData[col1]) -1)*100).rename(f'{col1}to{col2}')))\n",
    "\n",
    "# trainData = trainData.join(\n",
    "#     pd.concat([(((trainData[col2]/(trainData[col1]) -1)*100)\n",
    "#                 .rename(f'{col2}to{col1}'))\n",
    "#                for col1, col2 in combinations(sorted(numCols), 2)], 1))\n",
    "# testData = testData.join(\n",
    "#     pd.concat([(((testData[col2]/(testData[col1]) -1)*100)\n",
    "#                 .rename(f'{col2}to{col1}'))\n",
    "#                for col1, col2 in combinations(sorted(numCols), 2)], 1))\n",
    "# validationData = validationData.join(\n",
    "#     pd.concat([(((validationData[col2]/(validationData[col1]) -1)*100)\n",
    "#                 .rename(f'{col2}to{col1}'))\n",
    "#                for col1, col2 in combinations(sorted(numCols), 2)], 1))\n",
    "\n",
    "# trainData.head(2)\n",
    "# testData.head(2)\n",
    "# validationData.head(2)\n",
    "# trainData.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "newNumCols = trainData.columns[trainData.dtypes == 'float64']\n",
    "newNumCols = list(newNumCols)\n",
    "newNumCols.remove(labelColumn)\n",
    "\n",
    "featureColumn = newNumCols\n",
    "\n",
    "newNumCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "robustScaler = RobustScaler(quantile_range = (0.1,0.9))\n",
    "robustScaler = robustScaler.fit(trainData[newNumCols])\n",
    "trainData[newNumCols] =robustScaler.transform(trainData[newNumCols])\n",
    "testData[newNumCols] =robustScaler.transform(testData[newNumCols])\n",
    "validationData[newNumCols] =robustScaler.transform(validationData[newNumCols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.describe()\n",
    "testData.describe()\n",
    "validationData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for feature importance\n",
    "\n",
    "# LogRegModel = LogisticRegression()\n",
    "# LogRegModel.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "\n",
    "# # get importance\n",
    "# importance = LogRegModel.coef_[0]\n",
    "# # summarize feature importance\n",
    "# print(featureColumn)\n",
    "# for i,v in enumerate(importance):\n",
    "# \tprint('Feature: %0d, %s, Score: %.5f' % (i,featureColumn[i], v))\n",
    "    \n",
    "# # plot feature importance\n",
    "# plt.bar([x for x in range(len(importance))], importance)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-spelling",
   "metadata": {},
   "source": [
    "### Feature importance analysis\n",
    "for Tree Mortality Prediction based on Growth Patterns \n",
    "using Machine Learning Models  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART Feature Importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "DecTreeModel = DecisionTreeClassifier()\n",
    "RFModel = RandomForestClassifier()\n",
    "XGBModel = XGBClassifier()\n",
    "\n",
    "\n",
    "def FI_Analysis(crt_model):\n",
    "    crt_model.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "\n",
    "    # get importance\n",
    "    importance = crt_model.feature_importances_\n",
    "    # summarize feature importance\n",
    "    \n",
    "    print(type(crt_model).__name__)\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, %s, Score: %.5f' % (i,featureColumn[i], v))\n",
    "\n",
    "    # plot feature importance\n",
    "    plt.bar([x for x in range(len(importance))], importance)\n",
    "    plt.show()\n",
    "    return importance\n",
    "\n",
    "print(featureColumn)    \n",
    "importances = [FI_Analysis(crt_model) for crt_model in [DecTreeModel, RFModel, XGBModel]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [type(crt_model).__name__ for crt_model in [DecTreeModel, RFModel, XGBModel]] \n",
    "imps = pd.DataFrame.from_dict(dict(zip(model_names, importances))).set_index([featureColumn])\n",
    "\n",
    "imps\n",
    "\n",
    "# plt.pcolor(imps)\n",
    "# plt.yticks(np.arange(0.5, len(imps.index), 1), imps.index)\n",
    "# plt.xticks(np.arange(0.5, len(imps.columns), 1), imps.columns)\n",
    "# plt.show()\n",
    "sns.heatmap(imps, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, roc_auc_score, make_scorer\n",
    "\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "SVC_model = svm.SVC(kernel='rbf', random_state=0, gamma=.1, C=100, probability=True)\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "def perm_FI(crt_model, crt_scorer=roc_auc_scorer):\n",
    "\n",
    "    crt_model.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "    \n",
    "    # perform permutation importance\n",
    "    results = permutation_importance(crt_model, trainData[featureColumn], trainData[labelColumn], scoring=crt_scorer)\n",
    "    \n",
    "    # get importance\n",
    "    importance = results.importances_mean\n",
    "    \n",
    "    print(type(crt_model).__name__)\n",
    "    # summarize feature importance\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, %s, Score: %.5f' % (i,featureColumn[i], v))\n",
    "\n",
    "\n",
    "    # plot feature importance\n",
    "    plt.bar([x for x in range(len(importance))], importance)\n",
    "    plt.show()\n",
    "    return importance\n",
    "\n",
    "print(featureColumn)   \n",
    "model_list = [DecTreeModel, RFModel, XGBModel, SVC_model]\n",
    "importances = [perm_FI(crt_model) for crt_model in model_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [type(crt_model).__name__ for crt_model in model_list] \n",
    "imps= pd.DataFrame.from_dict(dict(zip(model_names, importances))).set_index([featureColumn])\n",
    "\n",
    "imps\n",
    "\n",
    "sns.heatmap(imps, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, roc_auc_score, make_scorer\n",
    "\n",
    "cohen_kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "model_list = [DecTreeModel, RFModel, XGBModel, SVC_model]\n",
    "importances = [perm_FI(crt_model, cohen_kappa_scorer) for crt_model in model_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [type(crt_model).__name__ for crt_model in model_list] \n",
    "imps= pd.DataFrame.from_dict(dict(zip(model_names, importances))).set_index([featureColumn])\n",
    "\n",
    "imps\n",
    "\n",
    "sns.heatmap(imps, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
