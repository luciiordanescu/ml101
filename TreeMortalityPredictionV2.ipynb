{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educated-response",
   "metadata": {},
   "source": [
    "### Tree mortality prediction based on growth patterns\n",
    "\n",
    "data: [HF213](https://harvardforest1.fas.harvard.edu/exist/apps/datasets/showData.html?id=HF213)\n",
    "\n",
    "Use classification algorithms to predict A(live) or D(ead) labels in __mortality13__ and __mortality14__ columns using these features:  \n",
    " - spp: USDA Plants database species code  \n",
    " - dbh09: diameter at Breast Height (1.4m) in year 2009 (unit: centimeter / missing value: NA)  \n",
    " - dbh11: diameter at Breast Height (1.4m) in year 2011 (unit: centimeter / missing value: NA)  \n",
    " - dbh12: diameter at Breast Height (1.4m) in year 2012 (unit: centimeter / missing value: NA)  \n",
    " - dbh13: diameter at Breast Height (1.4m) in year 2013 (unit: centimeter / missing value: NA)  \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "##### Tree mortality prediction based on growth patterns using Machine Learning\n",
    "Forests are critically important for biodiversity and provide important health and economic benefits. Forests' response to climate change is however not very well understood. This project addresses the importance of tree growth rate for understanding and predicting tree mortality rates. It is hypothesized that trees' growth patterns across multiple species may have subtle changes when trees are under stress and that growth changes over time can be used as an early predictor for tree mortality. For tree growth we use Diameter at Breast Height (dbh) measurements collected 4, 2, 1 and 0 (during the year of tree death) years previous to tree death. For modelling, we trained data driven machine learning models for mortality prediction using data classification. Our results show that simple growth measurements can be used to predict mortality with a reasonable performance (>.65 Area under ROC curve on test data ).  Further studies should use larger datasets to see if the modelsâ€™ accuracy can be increased. Mortality prediction models can further be used to understand which species are most likely to exhibit change in growth patterns under stress, as well as understand how early the changes appear before death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library InteractiveShell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#\"all\" the instructions are printed\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pathlib, shutil, platform\n",
    "#pandas procesing tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    StratifiedShuffleSplit,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_COUNT = 10\n",
    "TRAIN_DATA = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983647cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# ! ls -la ./../data/harvardf/HF213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install on the fly libraries in the current jupiter notebook\n",
    "# !/opt/conda/bin/conda install -c anaconda seaborn pandas scikit-learn -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the image ploted under the cell will be shown\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# !ls -la ./../data/harvardf/HF213\n",
    "\n",
    "# !ls -la ./\n",
    "# !ls -la # standard\n",
    "\n",
    "# !ls -la /./../\n",
    "# !ls -la /.. # standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee921a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlib.Path(['.','..','data','harvardf','HF213'])\n",
    "\n",
    "# myData = pd.read_csv('./../data/harvardf/HF213/' + dataFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFileName='hf213-01-hf-inventory.csv'\n",
    "dataPathFull= pathlib.Path('../data/harvardf/HF213') / dataFileName\n",
    "myData = pd.read_csv(str(dataPathFull)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce63b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(myData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData.shape\n",
    "myData.head(3)\n",
    "myData.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myData.head(3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData.info()\n",
    "myData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drops empty rows\n",
    "# myData.dropna()\n",
    "\n",
    "# # this one should be zero\n",
    "# myData.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic descriptive statistics for numeric columns:\n",
    "myData.describe()\n",
    "myData.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3718d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myData.groupby('spp').size()\n",
    "myCols = ['spp', 'mortality13', 'dmg13']\n",
    "# dropna=False means do not drop the NA\n",
    "myData[myCols[0]].value_counts(dropna=False) \n",
    "myData[myCols[1]].value_counts(dropna=False)\n",
    "myData[myCols[2]].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData.pivot_table(index = [myCols[0]]\n",
    "                   , columns = myCols[1]\n",
    "                   , values =  myCols[2]\n",
    "                   , aggfunc=np.sum, fill_value='')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x= myData['spp'],label=\"spp Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    " myData['spp'].value_counts(dropna=False).loc['ACRU']\n",
    "# type(myData['spp'].value_counts(dropna=False).loc[lambda x : x==1])\n",
    "myData['spp'].value_counts(dropna=False).loc[lambda x : x==1]\n",
    "myData['spp'].value_counts(dropna=False).loc[lambda x : x==1].index\n",
    "myData['spp'].value_counts(dropna=False).loc[lambda x : x==1].index.tolist()\n",
    "\n",
    "def my_f(x):\n",
    "    return x==2\n",
    " \n",
    "myData['spp'].value_counts(dropna=False).loc[my_f].index\n",
    "myData['spp'].value_counts(dropna=False).loc[lambda x : x==2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData['spp']\n",
    "myData['spp'].value_counts(dropna=False) \n",
    "# myData['spp'].value_counts(dropna=False).loc[lambda x : x<MINIMUM_COUNT].index\n",
    "removeSPP = myData['spp'].value_counts(dropna=False).loc[lambda x : x<MINIMUM_COUNT].index.tolist()\n",
    "\n",
    "removeSPP\n",
    "\n",
    "# filteredData = myData.replace(dict.fromkeys(removeSPP, 'TooFew'))\n",
    "# filteredData['spp'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1,1,2,3,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumn_01=['spp', 'dbh09', 'dbh11', 'dbh12']\n",
    "# featureColumn_01=[ 'dbh09', 'dbh11', 'dbh12']\n",
    "labelColumn_01 = 'mortality13'\n",
    "featureColumn_02=['spp', 'dbh09', 'dbh11', 'dbh12', 'dbh13']\n",
    "# featureColumn_02=['dbh09', 'dbh11', 'dbh12', 'dbh13']\n",
    "labelColumn_02 = 'mortality14' \n",
    "\n",
    "labelColumn = labelColumn_02 # 'mortality14'\n",
    "featureColumn = featureColumn_02 #['spp', 'dbh09', 'dbh11', 'dbh12', 'dbh13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumn+[labelColumn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(featureColumn+[labelColumn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData = myData\n",
    "filteredDataML = filteredData[sorted(set(featureColumn+[labelColumn]))]\n",
    "\n",
    "filteredDataML.shape\n",
    "filteredDataML.head()\n",
    "filteredDataML[labelColumn].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDataML[labelColumn].value_counts(dropna=False)\n",
    "filteredDataML[labelColumn].isin(['D', 'A'])\n",
    "\n",
    "filteredDataML = filteredDataML[filteredDataML[labelColumn].isin(['D', 'A'])]\n",
    "filteredDataML.shape\n",
    "filteredDataML.head()\n",
    "\n",
    "\n",
    "filteredDataML[labelColumn].value_counts(dropna=False) # double check the result that filter data is filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py\n",
    "filteredDataML.columns\n",
    "# filteredDataML.columns.tolist()\n",
    "filteredDataML.dtypes == 'O'\n",
    "\n",
    "catCols = filteredDataML.columns[filteredDataML.dtypes == 'O'] # features that are string\n",
    "numCols = filteredDataML.columns[filteredDataML.dtypes == 'float64']#numerical data\n",
    "catCols # categorical colummns\n",
    "numCols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270168cd",
   "metadata": {},
   "source": [
    "### Short example for StratifiedShuffleSplit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb87d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "myss = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_DATA, random_state=1)# equal % in 2 buckets\n",
    "\n",
    "# initialize data of lists.\n",
    "data = {'Name':['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9'],\n",
    "        'marks':[0,   0,     0,    1,     1,   1,     1,   1,    1,     1]}\n",
    "  \n",
    "# Creates pandas DataFrame.\n",
    "df = pd.DataFrame(data, index =['r00','r10','r20','r30','r40','r50','r60','r70','r80','r90'])\n",
    "\n",
    "trainIdx, tstIdx = next(myss.split(df, df['marks']))\n",
    "sorted(trainIdx)\n",
    "sorted(tstIdx)\n",
    "\n",
    "df.index[sorted(tstIdx)]\n",
    "df.loc[df.index[sorted(tstIdx)]]\n",
    "# df.loc[df.index[sorted(tstIdx)], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratifySplit = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_DATA, random_state=1)# equal % in 2 buckets\n",
    "# filteredDataML[labelColumn]\n",
    "# filteredDataML\n",
    "trainIdx, tstIdx = next(stratifySplit.split(filteredDataML, filteredDataML[labelColumn]))# you call it with next\n",
    "# print(\"\\n Train:\", sorted(trainIdx))\n",
    "len(trainIdx)\n",
    "len(tstIdx)\n",
    "\n",
    "# filteredDataML.loc[filteredDataML.index.intersection(filteredDataML.index[trainIdx])].shape # this is the shape of training data\n",
    "# filteredDataML[filteredDataML.index.isin(filteredDataML.index[trainIdx])].shape # another way to get the shape of training data\n",
    "aa=filteredDataML.loc[filteredDataML.index[tstIdx]]\n",
    "aa.shape\n",
    "# stratifySplit = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_DATA, test_size=1-TRAIN_DATA, random_state=1)\n",
    "testIdx, validationIdx = next(stratifySplit.split(aa,  aa[labelColumn]))\n",
    "\n",
    "len(testIdx)\n",
    "len(validationIdx)\n",
    "filteredDataML.shape\n",
    "\n",
    "# testIdx=tstIdx[testIdx]\n",
    "# validationIdx=tstIdx[validationIdx]\n",
    "\n",
    "# print(\"\\n Test:\", sorted(testIdx))\n",
    "# print(\"\\nValidation:\", sorted(validationIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredDataML.index\n",
    "# trainData=filteredDataML.loc[filteredDataML.index.intersection(filteredDataML.index[trainIdx]),:]# filteredDataML values indicated by the filteredDataML.index[trainIdx] \n",
    "testData=aa.loc[aa.index[testIdx]]\n",
    "validationData = aa.loc[aa.index[validationIdx]]\n",
    "\n",
    "trainData=filteredDataML.loc[(filteredDataML.index[trainIdx])]# filteredDataML values indicated by the filteredDataML.index[trainIdx] \n",
    "\n",
    "\n",
    "filteredDataML[labelColumn].value_counts(dropna=False)\n",
    "trainData[labelColumn].value_counts(dropna=False) \n",
    "testData[labelColumn].value_counts(dropna=False) \n",
    "validationData[labelColumn].value_counts(dropna=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinalEncoder = OrdinalEncoder()\n",
    "ordinalEncoder.fit(filteredDataML[catCols])\n",
    "ordinalEncoder.categories_\n",
    "\n",
    "trainData[catCols] = ordinalEncoder.transform(trainData[catCols])\n",
    "testData[catCols] = ordinalEncoder.transform(testData[catCols])\n",
    "validationData[catCols] = ordinalEncoder.transform(validationData[catCols])\n",
    "\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer.fit(trainData[featureColumn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[featureColumn] = imputer.transform(trainData[featureColumn])\n",
    "testData[featureColumn] = imputer.transform(testData[featureColumn])\n",
    "validationData[featureColumn] = imputer.transform(validationData[featureColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model = svm.SVC(kernel='rbf', random_state=0, gamma=.1, C=100, probability=True)\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=50)\n",
    "RF_model = RandomForestClassifier(n_estimators=10, class_weight=dict({0:10000., 1:10.}))\n",
    "\n",
    "\n",
    "SVC_model.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "KNN_model.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "RF_model.fit(trainData[featureColumn], trainData[labelColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_prediction = SVC_model.predict(testData[featureColumn])\n",
    "KNN_prediction = KNN_model.predict(testData[featureColumn])\n",
    "RF_prediction  = RF_model.predict(testData[featureColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(SVC_prediction, testData[labelColumn])\n",
    "accuracy_score(KNN_prediction, testData[labelColumn])\n",
    "accuracy_score(RF_prediction, testData[labelColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(SVC_prediction, testData[labelColumn])\n",
    "confusion_matrix(KNN_prediction, testData[labelColumn])\n",
    "confusion_matrix(RF_prediction, testData[labelColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(SVC_prediction,  testData[labelColumn]))\n",
    "print(classification_report(KNN_prediction,  testData[labelColumn]))\n",
    "print(classification_report(RF_prediction,  testData[labelColumn]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-approach",
   "metadata": {},
   "source": [
    "#### SVM SVC ROC curve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_prediction_probs = SVC_model.predict_proba(testData[featureColumn])\n",
    "fpr, tpr, thresholds = roc_curve(testData[labelColumn], SVC_prediction_probs[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "import pylab as pl\n",
    "pl.clf()\n",
    "pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('Receiver operating characteristic')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model_linear = svm.SVC(kernel=\"linear\", C=100)\n",
    "\n",
    "# rfe = RFE(estimator=SVC_model_linear, n_features_to_select=1, step=1, verbose=2)\n",
    "# rfe.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "# ranking = rfe.ranking_\n",
    "\n",
    "# # # Plot  ranking\n",
    "# # plt.matshow(ranking, cmap=plt.cm.Blues)\n",
    "# # plt.colorbar()\n",
    "# # plt.title(\"feature importance RFE\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_features_to_select = 1  # Minimum number of features to consider\n",
    "# rfecv = RFECV(estimator=SVC_model_linear, step=1, cv=StratifiedKFold(2),\n",
    "#               scoring='accuracy',\n",
    "#               min_features_to_select=min_features_to_select)\n",
    "# rfecv.fit(trainData[featureColumn], trainData[labelColumn])\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "# plt.plot(range(min_features_to_select,\n",
    "#                len(rfecv.grid_scores_) + min_features_to_select),\n",
    "#          rfecv.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
